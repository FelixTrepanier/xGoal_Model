---
title: "xG Model"
author: "FT"
date: "15/08/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---
Data acquired using the NHL scraper developed by Evolving Hockey
https://github.com/evolvingwild/evolving-hockey

Model built on the example of an xG model by Matthew Barlowe 
https://rstudio-pubs-static.s3.amazonaws.com/311470_f6e88d4842da46e9941cc6547405a051.html 
(NHL Expected Goals Model - Matthew Barlowe)

# Set up
## Packages
```{r setup, include=FALSE}
using<-function(...) {
  libs<-unlist(list(...))
  req<-unlist(lapply(libs,require,character.only=TRUE))
  need<-libs[req==FALSE]
  if(length(need)>0){ 
    install.packages(need)
    lapply(need,require,character.only=TRUE)
  }
}

using('ggplot2','dplyr','rstudioapi','data.table','ggthemes','tidyr','lubridate','scales','grid','pracma','mlr','mclust','mlbench','kernlab','ranger','cmaes','e1071','devtools','readr','stringr','foreach','jsonlite','rvest','xml2','RCurl','xgboost','fastDummies','neuralnet')

if (Sys.getenv("JAVA_HOME")!="")
  Sys.setenv(JAVA_HOME="")

memory.limit(size=56000)

options(digits=5)

```

## Directory
```{r}
current_path<-getActiveDocumentContext()$path
setwd(dirname(current_path ))
print(getwd())

```

# Data
## Loading in data
```{r}
data.18_19<-readRDS(paste0(getwd(),'/Data/2018-2019_PbP_Data.rds'))
data.19_20<-readRDS(paste0(getwd(),'/Data/2019-2020_PbP_Data.rds'))

```

## Separating the different dataframes
```{r}
# game_info_df_new<-pbp_scrape$game_info_df           ## game information data
# pbp_base_new<-pbp_scrape$pbp_base                   ## main play-by-play data
# pbp_extras_new<-pbp_scrape$pbp_extras               ## extra play-by-play data
# player_shifts_new<-pbp_scrape$player_shifts         ## full player shifts data
# player_periods_new<-pbp_scrape$player_periods       ## player TOI sums per period
# roster_df_new<-pbp_scrape$roster_df                 ## roster data
# scratches_df_new<-pbp_scrape$scratches_df           ## scratches data
# event_summary_df_new<-pbp_scrape$events_summary_df  ## event summary data
# scrape_report<-pbp_scrape$report                    ## scrape report

df.pbp_18_19<-data.18_19$pbp_base
df.pbp_19_20<-data.19_20$pbp_base

df.pbp<-rbind(df.pbp_18_19,df.pbp_19_20)
```


## Data exploration
```{r}
unique(df.pbp$event_type)
# Fenwick events are GOAL,MISS,SHOT

unique(df.pbp$game_strength_state)
# E means Extra, so Ev5 is 6v5, but can also be the start of the game before other team gets on ice

unique(df.pbp$event_detail)
# When its a fenwick event, this is where the shot type is recorded

```

# Building the training dataset
## Adding basic flags and diff time
```{r}
df.pbp<-df.pbp%>%
  filter(game_period<5)%>%
  mutate(goal_flag = ifelse(event_type == 'GOAL', 1, 0),
         home_flag = ifelse(event_team == home_team, 1, 0),
         period_id = paste0(game_id,game_period))%>%
  group_by(period_id)%>%
  mutate(time_diff = game_seconds - lag(game_seconds))

df.pbp$time_diff[is.na(df.pbp$time_diff)]<-0

```

## Adding rebound and rush flag
```{r}
df.pbp<-df.pbp%>%
  mutate(rebound_flag = ifelse(time_diff<3
                               & event_type %in% c('SHOT','GOAL','MISS')
                               & lag(event_type) %in% c('SHOT','GOAL','MISS')
                               & event_team == lag(event_team), 1, 0),
         rush_flag = ifelse(time_diff<4
                            & lag(abs(coords_x))<25
                            & event_type %in% c('SHOT','GOAL','MISS'), 1, 0))%>%
  filter(event_type %in% c('SHOT','GOAL','MISS'))

df.pbp$rebound_flag[is.na(df.pbp$rebound_flag)]<-0
df.pbp$rush_flag[is.na(df.pbp$rush_flag)]<-0

```

## Adding shot location, angle, and distance
```{r}
df.pbp<-df.pbp%>%
  filter(coords_x != 'NA' & coords_y != 'NA')%>%
  mutate(coords_y = ifelse(coords_x < 0, -1*coords_y,coords_y),
         coords_x = abs(coords_x),
         shot_angle = (asin(abs(coords_y)/sqrt((87.95 - abs(coords_x))^2 + coords_y^2))*180)/3.14159,
         shot_angle = ifelse(abs(coords_x) > 88, 90 + (180-(90 + shot_angle)), shot_angle),
         shot_distance = sqrt((87.95 - abs(coords_x))^2 + coords_y^2))

```

## Looking at shot types closely
### Dealing with NA
```{r}
df.pbp%>%
  group_by(event_detail)%>%
  summarise(n())

df.pbp%>%
  filter(is.na(event_detail)==T)

# What are NA shot types? 14 goals out of 19 shots! Will be classified as "Other"

df.pbp$event_detail[is.na(df.pbp$event_detail)]<-'Other'
```

### Difference between Tip-In vs Deflected
```{r}
# I am a bit worried tip-in might be a deflected shot that was a goal
df.pbp%>%
  filter(event_detail %in% c('Tip-In','Deflected'))%>%
  group_by(event_detail)%>%
  summarize(goal_count = sum(goal_flag),
            goal_proportion = mean(goal_flag))

# No worries to have here. Maybe deflected is a pass that goes off a skate while tip in an intentionally deflected shot
```

## Removing empty net fenwick events
```{r}
df.pbp<-df.pbp%>%
  mutate(empty_net_goal_flag = ifelse((home_flag == 1
                                       & is.na(away_goalie) == T)
                                      |(home_flag == 0
                                       & is.na(home_goalie) == T),1,0))%>%
  filter(empty_net_goal_flag != 1)

```

## Adding game strength
```{r}
df.pbp%>%
  filter(game_strength_state %in% c('3v3','3v4','3v5','3vE',
                                    '4v3','4v4','4v5','4vE',
                                    '5v3','5v4','5v5','5vE',
                                    'Ev3','Ev4','Ev5','EvE'))%>%
  group_by(game_strength_state)%>%
  summarise(n())

df.pbp<-df.pbp%>%
  filter(game_strength_state %in% c('3v3','3v4','3v5','3vE',
                                    '4v3','4v4','4v5','4vE',
                                    '5v3','5v4','5v5','5vE',
                                    'Ev3','Ev4','Ev5','EvE'))%>%
  mutate(even_strength_flag = ifelse(game_strength_state %in% c('5v5','4v4','3v3'),1,0),
         empty_net_flag = ifelse((home_flag==1 & game_strength_state %in% c('Ev3','Ev4','Ev5'))
                                 |(home_flag==0 & game_strength_state %in% c('3vE','4vE','5vE')),
                                 1,0),
         power_play_flag = ifelse((home_flag==1 & game_strength_state %in% c('5v3','5v4','4v3'))
                                  |(home_flag==0 & game_strength_state %in% c('3v5','4v5','3v4')),
                                  1,0),
         short_handed_flag = ifelse((home_flag==1 & game_strength_state %in% c('3v5','4v5','3v4'))
                                    |(home_flag==0 & game_strength_state %in% c('5v3','5v4','4v3')),
                                    1,0))
```

## Separating the two seasons
```{r}
df.pbp_18_19<-df.pbp%>%
  filter(season == 20182019)

df.pbp_19_20<-df.pbp%>%
  filter(season == 20192020)

rm(df.pbp)
```

## Training set
```{r}
df.train<-df.pbp_18_19%>%
  select(period_id,
         event_index,
         event_detail,
         rebound_flag,
         rush_flag,
         coords_x,
         coords_y,
         shot_angle,
         shot_distance,
         even_strength_flag,
         empty_net_flag,
         power_play_flag,
         short_handed_flag,
         goal_flag)%>%
  mutate(event_detail = as.factor(gsub('-','_',event_detail)),
         goal_flag=as.factor(goal_flag))

df.train<-dummy_cols(df.train, select_columns = 'event_detail')

df.train.id<-df.train%>%
  select(period_id,
         event_index)

df.train<-df.train[,-c(1,2,3)]
```

## Test set
```{r}
df.test<-df.pbp_19_20%>%
  select(period_id,
         event_index,
         event_detail,
         rebound_flag,
         rush_flag,
         coords_x,
         coords_y,
         shot_angle,
         shot_distance,
         even_strength_flag,
         empty_net_flag,
         power_play_flag,
         short_handed_flag,
         goal_flag)%>%
  mutate(event_detail = as.factor(gsub('-','_',event_detail)),
         goal_flag=as.factor(goal_flag))

df.test<-dummy_cols(df.test, select_columns = 'event_detail')

df.test.id<-df.test%>%
  select(period_id,
         event_index)

df.test<-df.test[,-c(1,2,3)]

```

# Building the model
## Model training parameters
```{r}
# hyper_grid<-makeTuneControlGrid()
hyper_grid<-makeTuneControlRandom(maxit = 500L)

# rdesc<-makeResampleDesc('RepCV', folds = 5, reps = 20)
rdesc<-makeResampleDesc('CV', iters = 5)
task.train_xG<-makeClassifTask(data = df.train, target = 'goal_flag', positive = '1')
task.test_xG<-makeClassifTask(data = df.test, target = 'goal_flag', positive = '1')

```

## XGboost
### XGboost learner tuning
```{r}
# listLearners()$class

# getLearnerParamSet('classif.xgboost')

xg_boost.hyper_param<-makeParamSet(makeNumericParam('eta',lower=0.01,upper=0.2),
                                   makeNumericParam('lambda',lower=0,upper=2),
                                   makeNumericParam('alpha',lower=0,upper=2),
                                   makeNumericParam('max_delta_step',lower=1,upper=10),
                                   makeNumericParam('gamma',lower=0.01,upper=0.2),
                                   makeNumericParam('subsample',lower=0.5,upper=0.95),
                                   makeNumericParam('colsample_bytree',lower=0.5,upper=0.9),
                                   makeIntegerParam('min_child_weight',lower=1,upper=40),
                                   makeIntegerParam('max_depth',lower=3,upper=27),
                                   makeIntegerParam('nrounds',lower=50,upper=1000),
                                   makeIntegerParam('early_stopping_rounds',lower=25,upper=25))

# xg_boost.hyper_param<-makeParamSet(makeNumericParam('eta',lower=0.01,upper=0.15),
#                                    makeNumericParam('lambda',lower=0,upper=2),
#                                    makeNumericParam('alpha',lower=0,upper=1),
#                                    makeNumericParam('max_delta_step',lower=2,upper=9),
#                                    makeNumericParam('gamma',lower=0.05,upper=0.4),
#                                    makeNumericParam('subsample',lower=0.6,upper=0.95),
#                                    makeNumericParam('colsample_bytree',lower=0.6,upper=0.9),
#                                    makeIntegerParam('min_child_weight',lower=1,upper=30),
#                                    makeIntegerParam('max_depth',lower=3,upper=15),
#                                    makeIntegerParam('nrounds',lower=50,upper=600),
#                                    makeIntegerParam('early_stopping_rounds',lower=25,upper=25))

xg_boost.learner<-makeLearner('classif.xgboost',predict.type='prob')

xg_boost.tune<-tuneParams(xg_boost.learner, task = task.xG, resampling = rdesc, par.set = xg_boost.hyper_param, control = hyper_grid, measures = list(auc))

df.xg_boost.tune<-as.data.frame(xg_boost.tune$opt.path)

```

### XGboost learner final tune
```{r}
xg_boost.learner<-makeLearner('classif.xgboost',
                              par.vals = list('eta'=xg_boost.tune$x$eta,
                                              'lambda'=xg_boost.tune$x$lambda,
                                              'alpha'=xg_boost.tune$x$alpha,
                                              'max_delta_step'=xg_boost.tune$x$max_delta_step,
                                              'gamma'=xg_boost.tune$x$gamma ,
                                              'subsample'=xg_boost.tune$x$subsample,
                                              'colsample_bytree'=xg_boost.tune$x$colsample_bytree,
                                              'min_child_weight'=xg_boost.tune$x$min_child_weight,
                                              'max_depth'=xg_boost.tune$x$max_depth,
                                              'nrounds'=xg_boost.tune$x$nrounds,
                                              'early_stopping_rounds'=xg_boost.tune$x$early_stopping_rounds),
                              predict.type='prob')

# listMeasures('classif', properties = 'classif')
# xg_boost.CV<-resample(learner = xg_boost.learner, task = task.xG,resampling = rdesc)

```

### First look at the results on the train set from XGboost
```{r}
xg_boost.train_model<-train(xg_boost.learner,task.train_xG)

xg_boost.train_pred = getPredictionProbabilities(predict(xg_boost.train_model, task = task.train_xG))

head(xg_boost.train_pred)
mean(xg_boost.train_pred)

calculateConfusionMatrix(predict(xg_boost.train_model, task = task.train_xG))

ggplot()+
  geom_histogram(aes(x=xg_boost.train_pred))

```

### First look at the results on the test set from XGboost
```{r}
xg_boost.test_model<-train(xg_boost.learner,task.test_xG)

xg_boost.test_pred = getPredictionProbabilities(predict(xg_boost.test_model, task = task.test_xG))

head(xg_boost.test_pred)
mean(xg_boost.test_pred)

calculateConfusionMatrix(predict(xg_boost.test_model, task = task.test_xG))

ggplot()+
  geom_histogram(aes(x=xg_boost.test_pred))

df.pbp_19_20$xG_xgboost<-xg_boost.test_pred
# save.image(paste0(dirname(getwd()),'/xG.RData'))
# load(paste0(dirname(getwd()),'/xG.RData'))

```

## Neural networks
### Neural networks learner tuning
```{r}
# listLearners()$class

getLearnerParamSet('classif.neuralnet')

neural_net.hyper_param<-makeParamSet(makeNumericParam('eta',lower=0.01,upper=0.2),
                                   makeNumericParam('lambda',lower=0,upper=2),
                                   makeNumericParam('alpha',lower=0,upper=2),
                                   makeNumericParam('max_delta_step',lower=1,upper=10),
                                   makeNumericParam('gamma',lower=0.01,upper=0.2),
                                   makeNumericParam('subsample',lower=0.5,upper=0.95),
                                   makeNumericParam('colsample_bytree',lower=0.5,upper=0.9),
                                   makeIntegerParam('min_child_weight',lower=1,upper=40),
                                   makeIntegerParam('max_depth',lower=3,upper=27),
                                   makeIntegerParam('nrounds',lower=50,upper=1000),
                                   makeIntegerParam('early_stopping_rounds',lower=25,upper=25))

neural_net.learner<-makeLearner('classif.neuralnet',predict.type='prob')

neural_net.tune<-tuneParams(neural_net.learner, task = task.xG, resampling = rdesc, par.set = neural_net.hyper_param, control = hyper_grid, measures = list(auc))

df.neural_net.tune<-as.data.frame(neural_net.tune$opt.path)

```

### Neural networks learner final tune
```{r}
neural_net.learner<-makeLearner('classif.neuralnet',
                              par.vals = list('eta'=xg_boost.tune$x$eta,
                                              'lambda'=xg_boost.tune$x$lambda,
                                              'alpha'=xg_boost.tune$x$alpha,
                                              'max_delta_step'=xg_boost.tune$x$max_delta_step,
                                              'gamma'=xg_boost.tune$x$gamma ,
                                              'subsample'=xg_boost.tune$x$subsample,
                                              'colsample_bytree'=xg_boost.tune$x$colsample_bytree,
                                              'min_child_weight'=xg_boost.tune$x$min_child_weight,
                                              'max_depth'=xg_boost.tune$x$max_depth,
                                              'nrounds'=xg_boost.tune$x$nrounds,
                                              'early_stopping_rounds'=xg_boost.tune$x$early_stopping_rounds),
                              predict.type='prob')

```

### First look at the results on the train set from neural networks
```{r}
neural_net.train_model<-train(neural_net.learner,task.train_xG)

neural_net.train_pred = getPredictionProbabilities(predict(neural_net.train_model, task = task.train_xG))

head(neural_net.train_pred)
mean(neural_net.train_pred)

calculateConfusionMatrix(predict(neural_net.train_model, task = task.train_xG))

ggplot()+
  geom_histogram(aes(x=neural_net.train_pred))

```

### First look at the results on the test set from neural networks
```{r}
neural_net.test_model<-train(neural_net.learner,task.test_xG)

neural_net.test_pred = getPredictionProbabilities(predict(neural_net.test_model, task = task.test_xG))

head(neural_net.test_pred)
mean(neural_net.test_pred)

calculateConfusionMatrix(predict(neural_net.test_model, task = task.test_xG))

ggplot()+
  geom_histogram(aes(x=neural_net.test_pred))

df.pbp_19_20$xneural_net<-neural_net.test_pred
# save.image(paste0(dirname(getwd()),'/xG.RData'))
# load(paste0(dirname(getwd()),'/xG.RData'))

```


# Looking at the model's results
## Aggregating at the player/game level
```{r}

```

## Aggregating at the player/season level
```{r}

```

## Aggregating at the team/game level
```{r}

```

## Aggregating at the team/season level
```{r}

```

